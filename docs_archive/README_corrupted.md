# IRE Assignment 1 - Information Retrieval System# IRE Assignment 1 - SelfIndex Implementation



> **Complete Implementation of Boolean, TF, and TF-IDF Indexing with Elasticsearch Comparison****Information Retrieval Engine** following the `index_base.py` specification.



[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)> **Status:** âœ… All assignment requirements complete | ğŸ§¹ Clean & organized codebase  

[![Evaluation](https://img.shields.io/badge/Queries-256-green.svg)](queries/test_queries.txt)> **Indices:** 18 built (3 ranking Ã— 2 datastores Ã— 3 compressions)  

[![Indices](https://img.shields.io/badge/Indices-12+3ES-orange.svg)](indices/)> **Plots:** 8 generated (all required visualizations)  

[![Status](https://img.shields.io/badge/Status-Complete-success.svg)]()> **Skip Pointers:** Implemented with 5.72x speedup  

> **Evaluation:** 256 queries Ã— 18 indices Ã— 2 modes = 9,216 executions

---

## Index Format Specification

## ğŸ“‹ Table of Contents

All indices follow the format defined in `src/index_base.py`:

1. [Overview](#overview)

2. [Key Features & Achievements](#key-features--achievements)```

3. [Index Naming Convention](#index-naming-convention)SelfIndex_i{x}d{y}c{z}q{q}o{optim}

4. [Dataset](#dataset)```

5. [Project Structure](#project-structure)

6. [Setup & Installation](#setup--installation)### Parameters

7. [How to Run](#how-to-run)

8. [Implementation Details](#implementation-details)| Parameter | Symbol | Values | Description |

9. [Evaluation Results](#evaluation-results)|-----------|--------|--------|-------------|

10. [Visualization & Plots](#visualization--plots)| **Index Type** | `i{x}` | 1, 2, 3 | 1=Boolean, 2=TF (WordCount), 3=TF-IDF |

11. [Key Findings](#key-findings)| **Datastore** | `d{y}` | 1, 2, 3 | 1=Custom/JSON, 2=DB1, 3=DB2 |

| **Compression** | `c{z}` | 1, 2, 3 | 1=None, 2=Code, 3=Library |

---| **Query Mode** | `q{q}` | T, D | T=Term-at-a-time, D=Document-at-a-time |

| **Optimization** | `o{optim}` | 0, sp, th, es | 0=None, sp=Skip Pointers, th=Threshold, es=Early Stop |

## ğŸ¯ Overview

### Examples

This project implements a complete **Information Retrieval System** with multiple indexing strategies, compression techniques, and query processing modes. The implementation follows the `index_base.py` specification and includes comparison with Elasticsearch.- `SelfIndex_i1d1c1qTo0` = Boolean/JSON/None/TAT/NoOpt

- `SelfIndex_i2d1c1qTo0` = TF/JSON/None/TAT/NoOpt

### What We Built- `SelfIndex_i3d1c1qTo0` = TF-IDF/JSON/None/TAT/NoOpt



- âœ… **3 Index Types**: Boolean, TF (Term Frequency), TF-IDF## Quick Start

- âœ… **2 Datastores**: Custom JSON, SQLite

- âœ… **3 Compression Methods**: None, Elias-Fano, Zlib### 1. Build Index

- âœ… **2 Query Modes**: TAAT (Term-at-a-Time), DAAT (Document-at-a-Time)

- âœ… **1 Optimization**: Skip Pointers for faster Boolean queries```powershell

- âœ… **Elasticsearch Comparison**: 3 cache scenarios (COLD, WARM, MIXED)# Activate environment

- âœ… **Comprehensive Evaluation**: 256 diverse queries across all configurations.\env\Scripts\Activate.ps1



---# Build Boolean index

python build.py -x 1 -y 1 -z 1 -q T -optim 0

## ğŸ† Key Features & Achievements

# Build TF index

### Implementation Highlightspython build.py -x 2 -y 1 -z 1 -q T -optim 0



1. **Complete SelfIndex Implementation**# Build TF-IDF index

   - Built from scratch following `index_base.py` specificationpython build.py -x 3 -y 1 -z 1 -q T -optim 0

   - All 7 abstract methods implemented```

   - Support for multiple ranking algorithms

**Output:** `indices/SelfIndex_i{x}d{y}c{z}q{q}o{optim}.json`

2. **Advanced Optimizations**

   - **Skip Pointers**: 1.03x speedup in Boolean queries### 2. Evaluate Performance

   - **Elias-Fano Compression**: 3.97x space reduction

   - **Zlib Compression**: 2.48x space reduction```powershell

   - **TAAT vs DAAT**: Query mode comparison# Measure Artifacts A (Latency), B (Throughput), C (Memory)

python evaluate.py -x 1 -y 1 -z 1 -q T -optim 0

3. **Elasticsearch Integration**python evaluate.py -x 2 -y 1 -z 1 -q T -optim 0

   - Custom English analyzer (lowercase + stopwords + stemming)python evaluate.py -x 3 -y 1 -z 1 -q T -optim 0

   - Fair comparison methodology```

   - Multiple cache scenarios for realistic evaluation

**Output:** `results/eval_SelfIndex_i{x}d{y}c{z}q{q}o{optim}.json`

4. **Comprehensive Evaluation**

   - **12 SelfIndex configurations** evaluated### 3. Generate Plots

   - **3 Elasticsearch scenarios** tested

   - **256 universal queries** (covering all query types)```powershell

   - **3 artifacts measured**: Latency (A), Throughput (B), Memory (C)# Plot.C: Compare index types (x=1,2,3)

python plot.py --type C --compare-x

5. **Professional Visualizations**

   - 6 comparison plots generated# Plot.A: Latency comparison

   - Results table with all metricspython plot.py --type A --compare-x

   - Publication-ready 300 DPI images

# Plot.B: Throughput comparison

---python plot.py --type B --compare-x



## ğŸ“› Index Naming Convention# Plot.AB: Latency + Throughput

python plot.py --type AB --compare-x

All indices follow a **standardized naming format** defined in `src/index_base.py`:

# Plot.AC: Latency + Memory

```python plot.py --type AC --compare-x

{core}_i{x}d{y}c{z}o{optim}```

```

**Output:** `plots/Plot{TYPE}_{param}.png`

### Parameters

### 4. Query Index

| Parameter | Symbol | Values | Description |

|-----------|--------|--------|-------------|```powershell

| **Core** | `{core}` | SelfIndex, ESIndex | Index implementation type |# Single query

| **Index Type** | `i{x}` | 1, 2, 3 | 1=Boolean, 2=TF, 3=TF-IDF |python query.py -x 3 -y 1 -z 1 -q T -optim 0 --query "machine learning algorithms"

| **Datastore** | `d{y}` | 1, 2 | 1=Custom/JSON, 2=SQLite |

| **Compression** | `c{z}` | 1, 2, 3 | 1=None, 2=Elias-Fano, 3=Zlib |# Interactive mode

| **Optimization** | `o{optim}` | 0, sp | 0=None, sp=Skip Pointers |python query.py -x 3 -y 1 -z 1 -q T -optim 0 --interactive

```

**Note**: Query mode (`qTAAT` or `qDAAT`) is **NOT** part of the index identifier as it's a runtime parameter, not a build-time property.

## Assignment Requirements

### Examples

### Plot.C for x=n (Compare Index Types) âœ…

```bash

SelfIndex_i1d1c1o0     # Boolean, JSON, No Compression, No Optimization**Completed:** Boolean (x=1), TF (x=2), TF-IDF (x=3)

SelfIndex_i2d1c1o0     # TF, JSON, No Compression, No Optimization

SelfIndex_i3d1c1o0     # TF-IDF, JSON, No Compression, No Optimization```powershell

SelfIndex_i3d1c2o0     # TF-IDF, JSON, Elias-Fano, No Optimizationpython plot.py --type C --compare-x

SelfIndex_i3d1c3o0     # TF-IDF, JSON, Zlib, No Optimization```

SelfIndex_i3d2c1o0     # TF-IDF, SQLite, No Compression, No Optimization

SelfIndex_i1d1c1osp    # Boolean, JSON, No Compression, Skip Pointers### Plot.A for y=n (Compare Datastores) â³

```

**Status:** Only y=1 (JSON) implemented. Need y=2 (DB1), y=3 (DB2).

---

### Plot.AB for z=n (Compare Compressions) â³

## ğŸ“Š Dataset

**Status:** Only z=1 (None) implemented. Need z=2 (Code), z=3 (Library).

### Composition

### Plot.A for i (Optimizations) â³

- **Total Documents**: 100,000

- **Wikipedia Articles**: 50,000**Status:** Only i=0 (None) implemented. Need i=sp (Skip Pointers).

- **News Articles**: 50,000

### Plot.AC for q (Query Processing) â³

### Preprocessing

**Status:** Only q=T (TAT) implemented. Need q=D (DAT).

All documents undergo:

1. **Tokenization**: Split on whitespace and punctuation## Current Implementation Status

2. **Lowercasing**: Convert all text to lowercase

3. **Stopword Removal**: Remove common English stopwords### âœ… Completed (Phase 1-3)

4. **Stemming**: Porter Stemmer applied

| Configuration | Description | Disk | Latency | Throughput |

Preprocessed data stored in: `preprocessed/preprocessed_data.json`|---------------|-------------|------|---------|------------|

| `i1d1c1qTo0` | Boolean/JSON | 915 MB | 0.71 ms | 1,417 q/s |

### Test Queries| `i2d1c1qTo0` | TF/JSON | 1,010 MB | TBD | TBD |

| `i3d1c1qTo0` | TF-IDF/JSON | 1,033 MB | TBD | TBD |

**256 diverse queries** covering all types:

### â³ To Implement

| Query Type | Count | Percentage | Example |

|------------|-------|------------|---------|- **Datastores (y):** DB1 (PostgreSQL/Redis), DB2 (RocksDB)

| Single-term | 20 | 7.8% | `python` |- **Compression (z):** Simple code, Library (zlib)

| Multi-term | 123 | 48.0% | `machine learning algorithms` |- **Optimization (i):** Skip pointers, Thresholding, Early stopping

| Boolean | 99 | 38.7% | `python AND data OR science` |- **Query Mode (q):** Document-at-a-time

| Phrase | 21 | 8.2% | `PHRASE(neural networks)` |

| Complex | 10 | 3.9% | `(python OR java) AND (data AND science)` |## Project Structure



Location: `queries/test_queries.txt````

IRE_Assignment1/

---â”œâ”€â”€ build.py                # Build indices

â”œâ”€â”€ evaluate.py             # Evaluate performance

## ğŸ“ Project Structureâ”œâ”€â”€ plot.py                 # Generate plots

â”œâ”€â”€ query.py                # Interactive queries

```â”œâ”€â”€ queries.txt             # Test queries

IRE_Assignment1/â”œâ”€â”€ download_data.py        # Download datasets

â”œâ”€â”€ src/                          # Core implementationâ”œâ”€â”€ README.md               # This file

â”‚   â”œâ”€â”€ index_base.py            # Base class specification (MODIFIED)â”œâ”€â”€ REPORT.md               # Assignment report

â”‚   â”œâ”€â”€ self_indexer.py          # SelfIndex implementationâ”œâ”€â”€ src/

â”‚   â”œâ”€â”€ es_indexer.py            # Elasticsearch wrapperâ”‚   â”œâ”€â”€ index_base.py       # Base specification (DO NOT MODIFY)

â”‚   â”œâ”€â”€ preprocessor.py          # Text preprocessingâ”‚   â”œâ”€â”€ self_indexer.py     # Boolean implementation (x=1)

â”‚   â””â”€â”€ data_loader.py           # Dataset loadingâ”‚   â”œâ”€â”€ self_indexer_x2.py  # TF implementation (x=2)

â”‚â”‚   â”œâ”€â”€ self_indexer_x3.py  # TF-IDF implementation (x=3)

â”œâ”€â”€ indices/                      # Built indicesâ”‚   â”œâ”€â”€ data_loader.py      # Dataset loading

â”‚   â”œâ”€â”€ SelfIndex_i1d1c1o0/â”‚   â””â”€â”€ preprocessor.py     # Text preprocessing

â”‚   â”œâ”€â”€ SelfIndex_i2d1c1o0/â”œâ”€â”€ indices/                # Built indices

â”‚   â”œâ”€â”€ SelfIndex_i3d1c1o0/â”‚   â”œâ”€â”€ SelfIndex_i1d1c1qTo0.json

â”‚   â”œâ”€â”€ SelfIndex_i3d1c2o0/â”‚   â”œâ”€â”€ SelfIndex_i2d1c1qTo0.json

â”‚   â”œâ”€â”€ SelfIndex_i3d1c3o0/â”‚   â””â”€â”€ SelfIndex_i3d1c1qTo0.json

â”‚   â”œâ”€â”€ SelfIndex_i3d2c1o0/â”œâ”€â”€ results/                # Evaluation results

â”‚   â”œâ”€â”€ SelfIndex_i1d1c1osp/â”‚   â””â”€â”€ eval_SelfIndex_*.json

â”‚   â””â”€â”€ esindex-v1.0/â”œâ”€â”€ plots/                  # Generated plots

â”‚â”œâ”€â”€ tests/                  # Unit tests

â”œâ”€â”€ results/                      # Evaluation results (JSON)â”œâ”€â”€ notebooks/              # Jupyter notebooks

â”‚   â”œâ”€â”€ eval_SelfIndex_*.json    # 12 SelfIndex evaluationsâ””â”€â”€ data/                   # Datasets

â”‚   â””â”€â”€ eval_esindex-v1.0_*.json # 3 ES evaluations    â”œâ”€â”€ wiki/               # Wikipedia articles

â”‚    â””â”€â”€ News_Datasets/      # News articles

â”œâ”€â”€ plots/                        # Visualizations```

â”‚   â”œâ”€â”€ plot_C_index_types.png   # Boolean vs TF vs TF-IDF

â”‚   â”œâ”€â”€ plot_A_datastores.png    # JSON vs SQLite## Dataset

â”‚   â”œâ”€â”€ plot_AB_compression.png  # Compression comparison

â”‚   â”œâ”€â”€ plot_A_optimizations.png # Skip pointers- **Wikipedia:** 100,000 articles

â”‚   â”œâ”€â”€ plot_AC_query_modes.png  # TAAT vs DAAT- **News:** 63,950 articles

â”‚   â”œâ”€â”€ plot_ES_comparison.png   # SelfIndex vs ES- **Total:** 163,950 documents

â”‚   â””â”€â”€ results_table.png        # Comprehensive results table- **Unique Terms:** 771,441

â”‚

â”œâ”€â”€ queries/                      # Query files## Complete Workflow Example

â”‚   â””â”€â”€ test_queries.txt         # 256 universal queries

â”‚```powershell

â”œâ”€â”€ preprocessed/                 # Preprocessed data# 1. Activate environment

â”‚   â””â”€â”€ preprocessed_data.json   # 100K documents.\env\Scripts\Activate.ps1

â”‚

â”œâ”€â”€ build.py                      # Index building script# 2. Build all three index types

â”œâ”€â”€ build_es.py                   # Elasticsearch index builderpython build.py -x 1 -y 1 -z 1 -q T -optim 0

â”œâ”€â”€ evaluate.py                   # Evaluation scriptpython build.py -x 2 -y 1 -z 1 -q T -optim 0

â”œâ”€â”€ query.py                      # Interactive query interfacepython build.py -x 3 -y 1 -z 1 -q T -optim 0

â”œâ”€â”€ generate_plots.py             # Visualization generator

â”‚# 3. Evaluate all three

â”œâ”€â”€ build_phase1.ps1             # PowerShell: Build all indicespython evaluate.py -x 1 -y 1 -z 1 -q T -optim 0

â”œâ”€â”€ evaluate_all.ps1             # PowerShell: Evaluate allpython evaluate.py -x 2 -y 1 -z 1 -q T -optim 0

â”‚python evaluate.py -x 3 -y 1 -z 1 -q T -optim 0

â””â”€â”€ README.md                     # This file

```# 4. Generate all plots

python plot.py --type A --compare-x

---python plot.py --type B --compare-x

python plot.py --type C --compare-x

## ğŸ”§ Setup & Installationpython plot.py --type AB --compare-x

python plot.py --type AC --compare-x

### Prerequisites

# 5. Query the best performing index

- Python 3.10+python query.py -x 3 -y 1 -z 1 -q T -optim 0 --interactive

- Elasticsearch 7.17+ (for ES comparison only)```

- Windows PowerShell (for batch scripts)

## Project Structure

### Step 1: Create Virtual Environment

```

```powershellIRE_Assignment1/

# Create environmentâ”œâ”€â”€ ğŸ“„ Core Scripts

python -m venv envâ”‚   â”œâ”€â”€ build.py                     # Build indices

â”‚   â”œâ”€â”€ evaluate.py                  # Performance evaluation

# Activateâ”‚   â”œâ”€â”€ generate_plots.py            # Generate visualizations

.\env\Scripts\Activate.ps1â”‚   â”œâ”€â”€ query.py                     # Query interface

```â”‚   â””â”€â”€ verify_plots.py              # Verify completeness

â”‚

### Step 2: Install Dependenciesâ”œâ”€â”€ ğŸ“ src/ - Implementation

â”‚   â”œâ”€â”€ index_base.py                # Base class

```powershellâ”‚   â”œâ”€â”€ self_indexer.py              # Boolean (x=1)

pip install -r requirements.txtâ”‚   â”œâ”€â”€ compressed_indexer.py        # TF (x=2) & TF-IDF (x=3)

```â”‚   â”œâ”€â”€ sqlite_indexer.py            # SQLite backend (y=3)

â”‚   â”œâ”€â”€ skip_pointers.py             # Skip pointers (o=sp)

**Key Dependencies**:â”‚   â””â”€â”€ compression/                 # VByte (z=2) & Zlib (z=3)

- `nltk` - Text preprocessingâ”‚

- `elasticsearch==7.17.12` - ES clientâ”œâ”€â”€ ğŸ“ indices/                      # 18 built indices

- `matplotlib` - Plottingâ”œâ”€â”€ ğŸ“ plots/                        # 8 visualization files

- `numpy` - Numerical operationsâ”œâ”€â”€ ğŸ“ results/                      # Evaluation results

â””â”€â”€ ğŸ“ tests/                        # Essential tests

### Step 3: Download NLTK Data```



```pythonSee `PROJECT_STRUCTURE.md` for detailed structure.

python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"

```## Testing



### Step 4: Start Elasticsearch (Optional)```powershell

# Run all tests

```powershellpytest tests/ -v

# Only needed for ES comparison

cd path\to\elasticsearch-7.17.x# Run specific test

.\bin\elasticsearch.batpytest tests/test_assignment_example.py -v

```pytest tests/test_boolean_parser.py -v

pytest tests/test_ranking.py -v

---```



## ğŸš€ How to Run## Requirements



### A. Build Individual Index- Python 3.10+

- NLTK (tokenization, stemming)

```powershell- NumPy (numerical operations)

python build.py -x <index_type> -y <datastore> -z <compression> -optim <optimization>- psutil (memory metrics)

```- matplotlib (visualizations)



**Examples**:## Documentation



```powershell- **`README.md`** - This file (main documentation)

# Build Boolean index- **`METRICS_DOCUMENTATION.md`** - **NEW! Comprehensive metrics & beautiful plots guide**

python build.py -x 1 -y 1 -z 1 -optim 0- **`PROJECT_STRUCTURE.md`** - Detailed project structure

- **`ASSIGNMENT_PLOTS_COMPLETE.md`** - Plot completion status

# Build TF-IDF index with Elias compression- **`CLEANUP_REPORT.md`** - Cleanup actions taken

python build.py -x 3 -y 1 -z 2 -optim 0- **`src/index_base.py`** - Base class specification



# Build Boolean index with Skip Pointers## ğŸ“Š Performance Metrics & Visualizations

python build.py -x 1 -y 1 -z 1 -optim sp

```### **NEW! Beautiful Comprehensive Plots** ğŸ¨



### B. Build All Indices (PowerShell Script)Run `python generate_beautiful_plots.py` to generate:



```powershell1. **Dashboard_Performance_Overview.png** (499 KB)

.\build_phase1.ps1   - 4-panel comprehensive dashboard

```   - Latency, throughput, index size, P95/P99 metrics



This builds all 7 SelfIndex configurations automatically.2. **Latency_Distribution_Analysis.png** (234 KB)

   - Average, P95, P99 latencies

### C. Build Elasticsearch Index   - TAAT vs DAAT comparison



```powershell3. **Compression_Analysis.png** (624 KB)

python build_es.py   - Size vs speed tradeoff

```   - Compression ratios & efficiency table



Builds `esindex-v1.0` with 100K documents.4. **Datastore_Comparison.png** (263 KB)

   - JSON vs SQLite performance

### D. Evaluate Single Index

5. **TAAT_vs_DAAT_Comparison.png** (271 KB)

```powershell   - Algorithm speedup factors

python evaluate.py <index_id> <query_mode>

```### **Metrics Tracked & Saved:**



**Examples**:All metrics saved in `plots/comprehensive_metrics.csv`:

- âœ… **Latency**: avg, median, p95, p99, min, max (ms)

```powershell- âœ… **Throughput**: QPS (queries per second)

# Evaluate TF-IDF with TAAT- âœ… **Memory**: Index size (MB), RAM usage (MB)

python evaluate.py SelfIndex_i3d1c1o0 TAAT- âœ… **Performance**: TAAT vs DAAT speedups



# Evaluate TF-IDF with DAAT**Total Data:** 36 rows (18 configs Ã— 2 modes) with 13 metrics each

python evaluate.py SelfIndex_i3d1c1o0 DAAT

See **`METRICS_DOCUMENTATION.md`** for complete details!

# Evaluate Elasticsearch (COLD cache)

python evaluate.py esindex-v1.0 COLD## Assignment Completion Status

```

âœ… **All Requirements Met:**

### E. Evaluate All Configurations- âœ… 18 indices built (3 ranking Ã— 2 datastores Ã— 3 compressions)

- âœ… Full evaluation (256 queries Ã— 18 indices Ã— 2 modes)

```powershell- âœ… All required plots generated (8 original + 5 beautiful new ones = 13 total)

.\evaluate_all.ps1- âœ… Skip pointers implemented (5.72x speedup)

```- âœ… **ALL metrics tracked & saved** (latency, p95, p99, throughput, memory)

- âœ… Clean, organized codebase (52 files removed)

Evaluates all 12 SelfIndex configurations (both TAAT and DAAT modes).

**Key Results:**

### F. Interactive Querying- **Skip Pointers:** 5.72x average speedup for Boolean AND queries

- **Compression:** VByte 2-3x reduction, Zlib 5-8x reduction

```powershell- **TAAT vs DAAT:** DAAT 100x+ faster

python query.py <index_id> <query_mode>- **Index Types:** TF-IDF best for relevance, Boolean fastest

```- **Throughput:** Up to 392K QPS (DAAT Boolean)



**Example**:---



```powershell**Project Status:** âœ… Complete and production-ready  

python query.py SelfIndex_i3d1c1o0 DAAT**Last Updated:** October 25, 2025



# Interactive prompt:## License

>>> Enter query: python AND machine learning

>>> Results: [doc123, doc456, ...]MIT License - IRE Assignment 1

```

### G. Generate Visualizations

```powershell
python generate_plots.py
```

Creates all 6 comparison plots + results table.

---

## ğŸ”¬ Implementation Details

### Changes Made to `index_base.py`

We modified the base specification to:

1. **Fixed Identifier Format**
   ```python
   # BEFORE: Included query mode in identifier
   self.identifier_short = "{}_i{}d{}c{}q{}o{}".format(...)
   
   # AFTER: Query mode is runtime parameter
   self.identifier_short = "{}_i{}d{}c{}o{}".format(core, x, y, z, optim)
   ```

2. **Added Optimization Enum**
   ```python
   class Optimizations(Enum):
       Null = '0'          # No optimization
       Skipping = 'sp'     # BUILD-TIME: Skip pointers
       Thresholding = 'th' # RUNTIME: Score thresholding
       EarlyStopping = 'es' # RUNTIME: Early stopping
   ```

3. **Updated Documentation**
   - Clarified BUILD-TIME vs RUNTIME optimizations
   - Skip pointers affect index structure (part of identifier)
   - Thresholding/EarlyStopping are query-time strategies

### SelfIndex Implementation (`self_indexer.py`)

**Core Components**:

1. **Inverted Index Structure**
   ```python
   {
       "term1": {
           "df": 100,  # Document frequency
           "postings": [
               {"doc_id": "doc1", "tf": 5, "positions": [0, 10, 20]},
               {"doc_id": "doc2", "tf": 3, "positions": [5, 15]},
               ...
           ]
       },
       ...
   }
   ```

2. **Boolean Retrieval** (`i=1`)
   - Set intersection/union for AND/OR queries
   - Returns ALL matching documents
   - No ranking applied

3. **TF Ranking** (`i=2`)
   - Score = sum(term_frequency in document)
   - Top-k retrieval with early termination
   - Faster than Boolean for ranked queries

4. **TF-IDF Ranking** (`i=3`)
   - Score = sum(TF * log(N/DF))
   - Top-k retrieval with heap
   - Best relevance ranking

5. **Compression Methods**
   - **Elias-Fano** (`c=2`): Custom implementation for posting lists
   - **Zlib** (`c=3`): Standard library compression

6. **Skip Pointers** (`o=sp`)
   - Added to Boolean index postings
   - Enables skipping over irrelevant postings
   - 1.03x speedup observed

7. **Query Processing Modes**
   - **TAAT** (`qTAAT`): Process one term at a time
   - **DAAT** (`qDAAT`): Process one document at a time

### Elasticsearch Implementation (`es_indexer.py`)

**Configuration**:

```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_english": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase", "english_stop", "english_stemmer"]
        }
      }
    }
  }
}
```

**Fair Comparison Methodology**:

1. **COLD Cache**: Clear cache before every query
2. **WARM Cache**: Run queries twice, use second run
3. **MIXED**: Realistic scenario with partial caching

All ES queries use:
- Top-10 results only
- Single field search (`content`)
- No document fetching (IDs only)

---

## ğŸ“ˆ Evaluation Results

### Comprehensive Results Table

![Results Table](plots/results_table.png)

*All 15 evaluated configurations with latency, throughput, and memory metrics*

### Summary Statistics

**Best Average Latency**: Boolean TAAT (2.80ms) with Skip Pointers  
**Best P95 Latency**: TF TAAT (8.67ms)  
**Highest Throughput**: Boolean TAAT (357 QPS) with Skip Pointers  
**Smallest Disk**: Elias-Fano compression (164.2 MB, 3.97x reduction)

### Key Metrics by Configuration

| Metric | Boolean (i1) | TF (i2) | TF-IDF (i3) |
|--------|--------------|---------|-------------|
| **Avg Latency (TAAT)** | 2.91ms | 3.34ms | 3.63ms |
| **P95 Latency (TAAT)** | 11.13ms | 8.67ms | 9.47ms |
| **Throughput (TAAT)** | 344 QPS | 300 QPS | 275 QPS |
| **Disk Size** | 575 MB | 635 MB | 651 MB |

### Compression Impact

| Method | Disk Size | Compression Ratio | Latency Overhead |
|--------|-----------|-------------------|------------------|
| None | 651 MB | 1.0x | Baseline |
| Elias-Fano | 164 MB | **3.97x** | +1160% slower |
| Zlib | 263 MB | **2.48x** | +172% slower |

### SelfIndex vs Elasticsearch

| Scenario | Latency (P95) | Throughput | Disk Size |
|----------|---------------|------------|-----------|
| SelfIndex (DAAT) | 16.12ms | 157 QPS | 651 MB |
| ES COLD | 12.60ms | 98 QPS | 418 MB |
| ES WARM | 10.22ms | 220 QPS | 418 MB |

---

## ğŸ“Š Visualization & Plots

### Generated Plots

1. **plot_C_index_types.png** - Boolean vs TF vs TF-IDF
   - Compares 3 ranking algorithms
   - Metrics: Latency, Throughput, Memory

2. **plot_A_datastores.png** - JSON vs SQLite
   - Datastore comparison
   - Shows 15.5% overhead for SQLite

3. **plot_AB_compression.png** - Compression Methods
   - None vs Elias vs Zlib
   - Space-time tradeoff analysis

4. **plot_A_optimizations.png** - Skip Pointers
   - Before vs After optimization
   - 1.03x speedup visualization

5. **plot_AC_query_modes.png** - TAAT vs DAAT
   - Query processing comparison
   - P95/P99 latency analysis

6. **plot_ES_comparison.png** - SelfIndex vs Elasticsearch
   - Fair comparison (COLD/WARM)
   - Performance vs Storage tradeoff

All plots available in `plots/` directory at 300 DPI.

---

## ğŸ” Key Findings

### 1. Boolean vs Ranked Retrieval Paradox

**Observation**: Boolean has **highest throughput** (344 QPS) but **highest P95 latency** (11.13ms)

**Explanation**:
- **Average latency**: Boolean is fastest (2.91ms) â†’ High throughput
- **P95 latency**: Boolean is slowest (11.13ms) â†’ Poor tail performance
- **Reason**: Boolean has high variance
  - Simple queries (`python`) are very fast (<1ms)
  - Complex queries (`A AND B AND C AND D`) are very slow (>11ms)
  - TF/TF-IDF use early termination â†’ more consistent performance

**P95/Average Ratio**:
- Boolean: **3.83x** (high variance)
- TF: **2.60x** (consistent)
- TF-IDF: **2.61x** (consistent)

### 2. Compression Tradeoff

Elias-Fano gives **best compression** (3.97x) but at a **severe latency cost** (+1160%)

**Recommendation**:
- Use Elias for **archival storage** (disk-limited)
- Use Zlib for **moderate compression** (2.48x, +172% latency)
- Use None for **latency-critical** applications

### 3. TAAT vs DAAT

**TAAT is 70% faster** than DAAT in our implementation:
- TAAT: 9.47ms P95
- DAAT: 16.12ms P95

**Reason**: TAAT benefits from better cache locality for TF-IDF scoring

### 4. Skip Pointers Impact

**Modest but measurable improvement**:
- Latency: 11.13ms â†’ 10.80ms (3.1% faster)
- Throughput: 344 QPS â†’ 357 QPS (3.8% higher)

**Worth implementing** for Boolean-heavy workloads.

### 5. Elasticsearch Comparison

ES **WARM** cache outperforms SelfIndex DAAT:
- ES WARM: 10.22ms P95, 220 QPS
- SelfIndex DAAT: 16.12ms P95, 157 QPS

**BUT** ES **COLD** cache is comparable:
- ES COLD: 12.60ms P95, 98 QPS (slower throughput)

**Conclusion**: For in-memory workloads, SelfIndex is competitive. ES shines with caching.

---

## ğŸ“ Files & Scripts Reference

### Core Scripts

| File | Purpose | Usage |
|------|---------|-------|
| `build.py` | Build single index | `python build.py -x 1 -y 1 -z 1 -optim 0` |
| `build_es.py` | Build ES index | `python build_es.py` |
| `evaluate.py` | Evaluate index | `python evaluate.py <index_id> <mode>` |
| `query.py` | Interactive queries | `python query.py <index_id> <mode>` |
| `generate_plots.py` | Create visualizations | `python generate_plots.py` |

### PowerShell Automation

| Script | Purpose |
|--------|---------|
| `build_phase1.ps1` | Build all 7 SelfIndex configurations |
| `evaluate_all.ps1` | Evaluate all indices (TAAT + DAAT) |

### Documentation

| File | Content |
|------|---------|
| `README.md` | This comprehensive guide |
| `ELASTICSEARCH_EVALUATION_GUIDE.md` | ES setup & methodology |
| `OPTIMIZATION_GUIDE.md` | Performance tuning tips |

---

## ğŸ“ Assignment Requirements Checklist

- [x] **Index Types**: Boolean, TF, TF-IDF implemented
- [x] **Datastores**: Custom JSON and SQLite
- [x] **Compression**: Elias-Fano and Zlib
- [x] **Query Modes**: TAAT and DAAT
- [x] **Optimizations**: Skip Pointers for Boolean
- [x] **Elasticsearch**: Built and evaluated (3 scenarios)
- [x] **Evaluation**: 256 queries across all configurations
- [x] **Artifacts**: Latency (A), Throughput (B), Memory (C)
- [x] **Plots**: 6 comparison plots generated
- [x] **Documentation**: Comprehensive README
- [x] **Results**: JSON files for all evaluations

---

## ğŸ¤ Contributors

**Vaibhav** - Full Implementation

---

## ğŸ“„ License

This project is part of an academic assignment for Information Retrieval course.

---

## ğŸ”— Quick Links

- [Results Table](plots/results_table.png)
- [Evaluation Files](results/)
- [Plots Directory](plots/)
- [Source Code](src/)

---


